{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8016887,"sourceType":"datasetVersion","datasetId":4723434},{"sourceId":8051004,"sourceType":"datasetVersion","datasetId":4747861}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Augment 10% of the CIFAR10 data n times and train MobileNetV2","metadata":{}},{"cell_type":"markdown","source":"## Set Random Seeds","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport random\nimport os\n\n# 1. Set the Python built-in random seed\nrandom.seed(123)\n\n# 2. Set the NumPy random seed\nnp.random.seed(123)\n\n# 3. Set the TensorFlow random seed\ntf.random.set_seed(123)\n\n# 4. Optionally, for some operations that are hardware-dependent and can introduce randomness\nos.environ['PYTHONHASHSEED'] = str(123)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:20:58.499192Z","iopub.execute_input":"2024-04-07T06:20:58.500075Z","iopub.status.idle":"2024-04-07T06:21:10.589111Z","shell.execute_reply.started":"2024-04-07T06:20:58.500041Z","shell.execute_reply":"2024-04-07T06:21:10.588325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Code (Adapted from .py file)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.metrics import Precision, Recall\n\n# Parameters\nbatch_size = 32\nimg_height = 96  # Resize the image\nimg_width = 96   # Resize the image\n\n# Load the custom training dataset\ntrain_ds = image_dataset_from_directory(\n    '/kaggle/input/10-of-cifar-10/filtered_cifar10_train/10',\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size,\n    label_mode='categorical'  # Ensure labels are one-hot encoded\n)\n\n# Load the test dataset to use as a validation set to ensure consistency across all experiments\nval_ds = image_dataset_from_directory(\n    '/kaggle/input/cifar10/cifar10/test',\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size,\n    label_mode='categorical'  # Ensure labels are one-hot encoded\n)\n\n# Define the data augmentation pipeline\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    tf.keras.layers.RandomRotation(0.3),\n    tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n])\n\n# Function to apply data augmentation n times\ndef augment_n_times(dataset, n):\n    augmented_datasets = [dataset]  # Start with the original dataset\n    for _ in range(n):\n        # Apply augmentation and add the augmented dataset to the list\n        augmented_datasets.append(dataset.map(lambda x, y: (data_augmentation(x, training=True), y)))\n    return augmented_datasets\n\n# Set i to the number of times to augment the dataset\ni = 0\n\n# Get the original + augmented datasets\naugmented_datasets = augment_n_times(train_ds, i)\n\n# Combine all datasets in the list into one\ncombined_dataset = augmented_datasets[0]\nfor augmented_dataset in augmented_datasets[1:]:\n    combined_dataset = combined_dataset.concatenate(augmented_dataset)\n\ndef normalize_img(image, label):\n    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n    return tf.cast(image, tf.float32) / 255., label\n\n# Apply the normalization to the training and validation datasets\ncombined_train_ds = combined_dataset.map(normalize_img)\nval_ds = val_ds.map(normalize_img)\n\n# Configure the dataset for performance\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ncombined_train_ds = combined_train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n# Check how big the dataset is\nprint(f\"Number of batches in the original pre-augmentation training dataset: {len(train_ds)}\")\nprint(f\"Number of batches in the combined training dataset: {len(combined_train_ds)}\")\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\n# Define the CNN model using MobileNetV2\ndef create_model():\n    base_model = MobileNetV2(input_shape=(img_height, img_width, 3),\n                             include_top=False,  # Do not include the top (final FC) layer\n                             weights=None)  # Do not use pre-trained weights\n    base_model.trainable = True  # Train from scratch\n\n    inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n    x = base_model(inputs, training=True)\n    x = layers.GlobalAveragePooling2D()(x)\n    outputs = layers.Dense(10, activation='softmax')(x)  # CIFAR-10 has 10 classes\n    model = models.Model(inputs, outputs)\n\n    model.compile(optimizer='adam',  \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy', Precision(), Recall()])\n    return model\n\nif __name__ == '__main__':\n    model = create_model()\n    history = model.fit(combined_train_ds, epochs=100, validation_data=val_ds)  \n\n    with open('/kaggle/working/training_metrics.txt', 'w') as file:\n        file.write('Epoch,Train Loss,Validation Loss,Train Accuracy,Validation Accuracy,Precision,Recall\\n')\n        for i in range(len(history.history['loss'])):\n            # Ensure precision and recall are formatted to string with desired precision, e.g., \"{:.4f}\".format(...)\n            train_loss = history.history['loss'][i]\n            val_loss = history.history['val_loss'][i]\n            train_accuracy = history.history['accuracy'][i]\n            val_accuracy = history.history['val_accuracy'][i]\n            precision = history.history['precision'][i]\n            recall = history.history['recall'][i]\n\n            file.write(f\"{i+1},{train_loss},{val_loss},{train_accuracy},{val_accuracy},{precision},{recall}\\n\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T06:21:10.590601Z","iopub.execute_input":"2024-04-07T06:21:10.591111Z","iopub.status.idle":"2024-04-07T06:29:01.817190Z","shell.execute_reply.started":"2024-04-07T06:21:10.591085Z","shell.execute_reply":"2024-04-07T06:29:01.816471Z"},"trusted":true},"execution_count":null,"outputs":[]}]}