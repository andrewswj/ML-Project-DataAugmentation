{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-07T06:47:25.343558Z","iopub.status.busy":"2024-04-07T06:47:25.342719Z","iopub.status.idle":"2024-04-07T06:47:31.132258Z","shell.execute_reply":"2024-04-07T06:47:31.131509Z","shell.execute_reply.started":"2024-04-07T06:47:25.343525Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import random\n","import os\n","\n","# 1. Set the Python built-in random seed\n","random.seed(123)\n","\n","# 2. Set the NumPy random seed\n","np.random.seed(123)\n","\n","# 3. Set the TensorFlow random seed\n","tf.random.set_seed(123)\n","\n","# 4. For some operations that are hardware-dependent and can introduce randomness\n","os.environ['PYTHONHASHSEED'] = str(123)\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T06:47:31.991917Z","iopub.status.busy":"2024-04-07T06:47:31.991290Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras.metrics import Precision, Recall\n","\n","# Parameters\n","batch_size = 32\n","img_height = 96  # Resize the image\n","img_width = 96   # Resize the image\n","\n","# Load the custom training dataset\n","train_ds = image_dataset_from_directory(\n","    '/kaggle/input/cifar10/cifar10/train',\n","    seed=123,\n","    image_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    label_mode='categorical'  # Ensure labels are one-hot encoded\n",")\n","\n","# Load the test dataset to use as a validation set to ensure consistency across all experiments\n","val_ds = image_dataset_from_directory(\n","    '/kaggle/input/cifar10/cifar10/test',\n","    seed=123,\n","    image_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    label_mode='categorical'  # Ensure labels are one-hot encoded\n",")\n","\n","\n","# Define the data augmentation pipeline\n","data_augmentation = tf.keras.Sequential([\n","    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n","    tf.keras.layers.RandomRotation(0.3),\n","    tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n","])\n","\n","# Function to apply data augmentation n times\n","def augment_n_times(dataset, n):\n","    augmented_datasets = [dataset]  # Start with the original dataset\n","    for _ in range(n):\n","        # Apply augmentation and add the augmented dataset to the list\n","        augmented_datasets.append(dataset.map(lambda x, y: (data_augmentation(x, training=True), y)))\n","    return augmented_datasets\n","\n","# Set i to 0 so we do not augment the dataset if we are using the full training set.\n","i = 0\n","\n","# Get the original + augmented datasets\n","augmented_datasets = augment_n_times(train_ds, i)\n","\n","# Combine all datasets in the list into one\n","combined_dataset = augmented_datasets[0]\n","for augmented_dataset in augmented_datasets[1:]:\n","    combined_dataset = combined_dataset.concatenate(augmented_dataset)\n","\n","def normalize_img(image, label):\n","    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n","    return tf.cast(image, tf.float32) / 255., label\n","\n","# Apply the normalization to the training and validation datasets\n","combined_train_ds = combined_dataset.map(normalize_img)\n","val_ds = val_ds.map(normalize_img)\n","\n","# Configure the dataset for performance\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","combined_train_ds = combined_train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","# Check how big the dataset is\n","print(f\"Number of batches in the original pre-augmentation training dataset: {len(train_ds)}\")\n","print(f\"Number of batches in the combined training dataset: {len(combined_train_ds)}\")\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","# Define the CNN model using MobileNetV2\n","def create_model():\n","    base_model = MobileNetV2(input_shape=(img_height, img_width, 3),\n","                             include_top=False,  # Do not include the top (final FC) layer\n","                             weights=None)  # Do not use pre-trained weights\n","    base_model.trainable = True  # Train from scratch\n","\n","    inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n","    x = base_model(inputs, training=True)\n","    x = layers.GlobalAveragePooling2D()(x)\n","    outputs = layers.Dense(10, activation='softmax')(x)  # CIFAR-10 has 10 classes\n","    model = models.Model(inputs, outputs)\n","\n","    model.compile(optimizer='adam',  \n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy', Precision(), Recall()])\n","    return model\n","\n","if __name__ == '__main__':\n","    model = create_model()\n","    history = model.fit(combined_train_ds, epochs=100, validation_data=val_ds)  \n","\n","    with open('/kaggle/working/training_metrics.txt', 'w') as file:\n","        file.write('Epoch,Train Loss,Validation Loss,Train Accuracy,Validation Accuracy,Precision,Recall\\n')\n","        for i in range(len(history.history['loss'])):\n","            # Ensure precision and recall are formatted to string with desired precision, e.g., \"{:.4f}\".format(...)\n","            train_loss = history.history['loss'][i]\n","            val_loss = history.history['val_loss'][i]\n","            train_accuracy = history.history['accuracy'][i]\n","            val_accuracy = history.history['val_accuracy'][i]\n","            precision = history.history['precision'][i]\n","            recall = history.history['recall'][i]\n","\n","            file.write(f\"{i+1},{train_loss},{val_loss},{train_accuracy},{val_accuracy},{precision},{recall}\\n\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4747861,"sourceId":8051004,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
